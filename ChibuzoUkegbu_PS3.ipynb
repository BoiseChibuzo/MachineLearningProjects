{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHibuzo Ukegbu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuWANcq7m1KQ"
   },
   "source": [
    "# CS534 Homework 3\n",
    "\n",
    "Put your homework in the directory with your name. Please mentionin this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides course notes you or others took during lecture. the required text (CLRS) websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems. Deadline 04/27/2020\n",
    "\n",
    "Please provide an answer to the following question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support,confusion_matrix,accuracy_score\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ1pdl7vOeeE"
   },
   "source": [
    "# Question 1 (10 pts)\n",
    "Write the explicit constraints (without using any vectorial notation, as a summation of single variables multiplied by a constant + bias term) of the Support Vector Machine to classify correctly iris dataset (Iris-Versicolor vs. others). In particular use 5 points in Iris-Versicolor, 2 points for iris-setosa, and 3 points for iris Virginia. Please show the points you selected and after the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Versicolor points:\n",
      "[[7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]]\n",
      "\n",
      "Iris Setosa points:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]]\n",
      "\n",
      "Iris Virginica points:\n",
      "[[6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]]\n",
      "\n",
      "The explicit contraints of the SVM to classify iris dataset correctly are as follows:\n",
      "7.0*a0 + b >= 1\n",
      "3.2*a1 + b >= 1\n",
      "4.7*a2 + b >= 1\n",
      "1.4*a3 + b >= 1\n",
      "6.4*a0 + b >= 1\n",
      "3.2*a1 + b >= 1\n",
      "4.5*a2 + b >= 1\n",
      "1.5*a3 + b >= 1\n",
      "6.9*a0 + b >= 1\n",
      "3.1*a1 + b >= 1\n",
      "4.9*a2 + b >= 1\n",
      "1.5*a3 + b >= 1\n",
      "5.5*a0 + b >= 1\n",
      "2.3*a1 + b >= 1\n",
      "4.0*a2 + b >= 1\n",
      "1.3*a3 + b >= 1\n",
      "6.5*a0 + b >= 1\n",
      "2.8*a1 + b >= 1\n",
      "4.6*a2 + b >= 1\n",
      "1.5*a3 + b >= 1\n",
      "5.1*a0 + b <= -1\n",
      "3.5*a1 + b <= -1\n",
      "1.4*a2 + b <= -1\n",
      "0.2*a3 + b <= -1\n",
      "4.9*a0 + b <= -1\n",
      "3.0*a1 + b <= -1\n",
      "1.4*a2 + b <= -1\n",
      "0.2*a3 + b <= -1\n",
      "6.3*a0 + b <= -1\n",
      "3.3*a1 + b <= -1\n",
      "6.0*a2 + b <= -1\n",
      "2.5*a3 + b <= -1\n",
      "5.8*a0 + b <= -1\n",
      "2.7*a1 + b <= -1\n",
      "5.1*a2 + b <= -1\n",
      "1.9*a3 + b <= -1\n",
      "7.1*a0 + b <= -1\n",
      "3.0*a1 + b <= -1\n",
      "5.9*a2 + b <= -1\n",
      "2.1*a3 + b <= -1\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "classes =  list(datasets.load_iris()['target_names'])\n",
    "\n",
    "print(\"Iris Versicolor points:\")\n",
    "data_vers = X[y==classes.index(\"versicolor\")][:5]\n",
    "print(data_vers)\n",
    "print()\n",
    "print(\"Iris Setosa points:\")\n",
    "data_seto = X[y==classes.index(\"setosa\")][:2]\n",
    "print(data_seto)\n",
    "print()\n",
    "print(\"Iris Virginica points:\")\n",
    "data_virg = X[y==classes.index(\"virginica\")][:3]\n",
    "print(data_virg)\n",
    "\n",
    "# Applying the constraints\n",
    "print(\"\\nThe explicit contraints of the SVM to classify iris dataset correctly are as follows:\")\n",
    "# Versicolor\n",
    "for i in range(data_vers.shape[0]):\n",
    "    for a, feat in enumerate(data_vers[i, :]):\n",
    "        print(\"{}*{} + \".format(feat, \"a\"+str(a)), end=\"\")\n",
    "        print(\"b >= 1\")\n",
    "# Setosa\n",
    "for i in range(data_seto.shape[0]):\n",
    "    for a, feat in enumerate(data_seto[i, :]):\n",
    "        print(\"{}*{} + \".format(feat, \"a\"+str(a)), end=\"\")\n",
    "        print(\"b <= -1\")\n",
    "# Virginica\n",
    "for i in range(data_virg.shape[0]):\n",
    "    for a, feat in enumerate(data_virg[i, :]):\n",
    "        print(\"{}*{} + \".format(feat, \"a\"+str(a)), end=\"\")\n",
    "        print(\"b <= -1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9_7-Wkkm1KR"
   },
   "source": [
    "# Question 2 (15 pts)\n",
    "\n",
    "Implement the ADABoost algorithm by using the scikit implementation of the logistic regression. Evaluate the result on a real dataset between a single logistic regression and AdaBoost (use K-Fold cross validation).\n",
    "\n",
    "This links can be helpful: http://rob.schapire.net/papers/explaining-adaboost.pdf and https://en.wikipedia.org/wiki/AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.c_[cancer['data'], cancer['target']],\n",
    "                  columns= np.append(cancer['feature_names'], ['target']))\n",
    "X=cancer['data']\n",
    "y=cancer['target']\n",
    "\n",
    "def out(s):\n",
    "    if s==1:\n",
    "        return 1\n",
    "    return -1\n",
    "y=np.array([out(d) for d in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_train(train_data,classes,iterations_num):\n",
    "    #calculate number of samples in train data\n",
    "    abselon=.000000000001 #small value to add to dominator at division\n",
    "    best_classifiers=[]#list to save trained classifier\n",
    "    calssifier_significance=[]#list to save significance of each classifier\n",
    "    samples_num=train_data.shape[0]#number of samples in train data\n",
    "    features_num=train_data.shape[1]#number of features in train data\n",
    "    #define 1d numpy array of length == number of samples for saving sample weights\n",
    "    weights=np.zeros(samples_num)#array to save the weight of each sample initialized to zero\n",
    "    indices=np.zeros(samples_num)#array to save the index of each sample initialized to zero\n",
    "    #initialize all weights equall with 1/samples_num\n",
    "    weights=np.array([1/samples_num for d in weights])\n",
    "    #add the index of each sample \n",
    "    for k in range(0,samples_num):\n",
    "        indices[k] =k \n",
    "    #loop around iterations \n",
    "    for i in range(0,iterations_num):\n",
    "        #train logistic regression\n",
    "        clf = LogisticRegression(random_state=0).fit(train_data,classes)\n",
    "        #pred the output for each sample \n",
    "        pred=clf.predict(train_data)\n",
    "        error=0\n",
    "        #calculate the error\n",
    "        for k in range(0,samples_num):\n",
    "            if (pred[k]!=y[k]):\n",
    "                error=error+weights[k]\n",
    "        #calculate significance\n",
    "        significance=.5*np.log((1-error)/(error+abselon))\n",
    "        #add classifier to list \n",
    "        best_classifiers.append(clf)\n",
    "        #add significance to list \n",
    "        calssifier_significance.append(significance)\n",
    "        #update the weights \n",
    "        for h in range(0,samples_num):\n",
    "            if (pred[h]!=y[h]):\n",
    "                weights[h]=weights[h]*np.exp(significance)\n",
    "            else:\n",
    "                weights[h]=weights[h]*np.exp(-1*significance)\n",
    "        weights=weights/sum(weights)\n",
    "        #based on the weights and indices we choose new dataset that sample with higher weights has most likelihood to be repeated in array\n",
    "        indices=np.random.choice(indices,p=weights, size=(samples_num,),replace=True).astype(int)\n",
    "        #update train data and classes with new indicies \n",
    "        train_data=train_data[indices]\n",
    "        classes=classes[indices]\n",
    "        #update the weights to be equal \n",
    "        weights=np.array([1/samples_num for d in weights])\n",
    "        #make the indices for new array to be between 0-sample_num\n",
    "        for k in range(0,samples_num):\n",
    "            indices[k] =k\n",
    "    #return two lists one for classifiers and one for indecies\n",
    "    return calssifier_significance,best_classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_test(test_data,calssifier_significance,classifiers):\n",
    "    #calculate number of data samples\n",
    "    samples_num=test_data.shape[0]\n",
    "    #initialize array for predictions\n",
    "    predictions=np.zeros(samples_num)\n",
    "    #loop around samples_number\n",
    "    for i in range(0,samples_num):\n",
    "        #read one test sample\n",
    "        test_element=test_data[i,:].reshape(1, -1)\n",
    "        calssifier_significance_num=0\n",
    "        res=0\n",
    "        #loop around classifier\n",
    "        for obj in classifiers:\n",
    "            #use the prediction of each classifier\n",
    "            pred=obj.predict(test_element)\n",
    "            sig=calssifier_significance[calssifier_significance_num]\n",
    "            #multiply the prediciton of the classifier to its significance and add them to result\n",
    "            res=res+(pred*sig)\n",
    "            calssifier_significance_num=calssifier_significance_num+1\n",
    "            #check if the final result >or < zero to take definal decision\n",
    "        if(res>0):\n",
    "            predictions[i]=1\n",
    "        else:\n",
    "            predictions[i]=-1\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868421052631579\n"
     ]
    }
   ],
   "source": [
    "calssifier_significance,best_classifiers=adaboost_train(X_train,y_train,7)\n",
    "predictions=adaboost_test(X_test,calssifier_significance,best_classifiers)\n",
    "accuracy=accuracy_score(y_test, predictions)    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(X_train,y_train)\n",
    "pred2=clf.predict(X_test)\n",
    "accuracy=accuracy_score(y_test, pred2)    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBCQhHD-m1KS"
   },
   "source": [
    "## Question 3 (10 pts)\n",
    "Use DBscan (try with different parameters) and K-means (K=3) on IRIS Dataset and discuss/compare the results with the iris ground truth.\n",
    "Please provide an explanation of why K-fold cross validation is not required for the comparison among these different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)\n",
    "X=data.iloc[:,[0,1,2,3]]\n",
    "y=data.loc[:,4]\n",
    "def out(s):\n",
    "    if s=='Iris-versicolor':\n",
    "        return 1\n",
    "    elif s=='Iris-setosa':\n",
    "        return 0\n",
    "    return 2\n",
    "y=np.array([out(d) for d in y])\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7514854021988338\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, max_iter=1000).fit(X)\n",
    "pred=kmeans.predict(X)\n",
    "accuracy=homogeneity_score(y, pred)    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters\n",
      "epsilon= 0.4\n",
      "min_samples= 2\n",
      "accuracy= 0.8555893610823645\n"
     ]
    }
   ],
   "source": [
    "eps=[.1,.2,.3,.4,.5,.6,.7,.8,.9,1,1.1,1.2,1.3,1.5,1.7,2]\n",
    "min_samples=[2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n",
    "best_accuracy=0\n",
    "best_eps=0\n",
    "best_samples=0\n",
    "for obj in eps:\n",
    "    for samples in min_samples:\n",
    "        pred = DBSCAN(eps=obj, min_samples=samples).fit_predict(X)\n",
    "        accuracy=homogeneity_score(y, pred)\n",
    "        if (accuracy>best_accuracy):\n",
    "            best_accuracy=accuracy\n",
    "            best_eps=obj\n",
    "            best_samples=samples\n",
    "print(\"Best parameters\")        \n",
    "print(\"epsilon=\",best_eps)\n",
    "print(\"min_samples=\",best_samples)\n",
    "print(\"accuracy=\",best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the homogenity of DBscan is better than k-means this means that the points of each class can be considered to have density based structure.\n",
    "Homogenity means each cluster contains only members of a single class. so if all the samples in the same class belonging to the same cluster the homogenity is 1 and if all samples in cluster each belongs to different classes\n",
    "We assume that there are M data samples, J different class labels, K clusters and $\\alpha_{ck}$ number of data-points belonging to the class c and cluster k Then the homogeneity h is given by the following\n",
    "$h=1-(\\frac{H(J,K)}{H(J)})$ \n",
    "\n",
    "where\n",
    "\n",
    "$H(J,K)=-\\sum \\limits _{k=1} ^{K}\\sum \\limits _{j=1} ^{J}\\frac{\\alpha_{jk}}{M}log(\\frac{\\alpha_{jk}}{{\\sum \\limits _{j=1} ^{J} {\\alpha_{jk}}}}) $\n",
    "\n",
    "where $H(J,K)$ is the conditional entropy of the classes given the cluster assignments\n",
    "\n",
    "$H(J)=-\\sum \\limits _{j=1} ^{J}\\frac{\\sum \\limits _{k=1} ^{K}\\alpha_{jk}}{C}log(\\frac{\\sum \\limits _{k=1} ^{K}\\alpha_{jk}}{J}) $\n",
    "\n",
    "$H(J)$ is the entropy of the classes \n",
    "\n",
    "\n",
    "K-fold cross validation is used in classification to prevent overfitting and choosing hyperparameters but in clustering we don't feed to the model the labels, and feeding all the train data at once would help the algorithm to calculate its parameters such as the mean of each class in k-means more accurately than applying each fold and calculating the parameters for each fold and then doing the average of the results of all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mHkf54Sm1KT"
   },
   "source": [
    "# Extra Points ( 5pts)\n",
    "An unbalanced dataset (e.g. 95% vs 5%) can be problematic even in the training phase. The learned function can be trivial, e.g. always predicting one class.\n",
    "A possible solution can have a weight for each point in the way that making a mistake in the minority class will coun more w.r.t. the other. Please redefine the likelihood of the logistic regression to consider these weights for each point. Please compute the log-likelihood and its derivatives.\n",
    "In addition, add to the negative log-likelihood the norm of W (sum of the square of each component) and compute the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood function for the logistic regression function is given as:\n",
    "\n",
    "$$\\prod_{i=1}^{m} P(y = 1 | x^{(i)})^{y^{(i)}} P(y = 0 | x^{(i)})^{1 - y^{(i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqC9w6tkm1KU"
   },
   "source": [
    "Without loss of generality, suppose that the class 1 has $k$ number of observations and class 0 has $m - k$ number of observations such that $k > m-2$. Then, each observation in class 2 should be counted as $\\frac{k}{m-k}$ observation.\n",
    "\n",
    "For instance,in the data set of $m = 10$ observations, suppose the class 1 has $k = 8$ observations and class 0 has $m - k = 2$ observations. Then each observation in class 0 should be counted as $\\frac{8}{10-8} = 5$ observations to balance the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1GyJLFQK1uC"
   },
   "source": [
    "Then the likelihood function of the logistic regression function is:\n",
    "\n",
    "$$\\prod_{i=1}^{m} P(y = 1 | x^{(i)})^{y^{(i)}} \\left( \\frac{k}{m-k} P(y = 0 | x^{(i)}) \\right)^{1 - y^{(i)}}$$\n",
    "\n",
    "and the log likelihood function of the logistic regression function would then be:\n",
    "\n",
    "$$\\sum_{i=1}^{m} y^{(i)} \\log P(y = 1 | x^{(i)}) + \\frac{k(1 - y^{(i)})}{m-k} \\log P(y = 0 | x^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ensure that the contribution to the log likelihood function is equally divided between two classes 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PS3Sp22.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
